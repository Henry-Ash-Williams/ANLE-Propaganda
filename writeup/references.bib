@article{martino2020survey,
  author     = {Giovanni Da San Martino and
                Stefano Cresci and
                Alberto Barr{\'{o}}n{-}Cede{\~{n}}o and
                Seunghak Yu and
                Roberto Di Pietro and
                Preslav Nakov},
  title      = {A Survey on Computational Propaganda Detection},
  journal    = {CoRR},
  volume     = {abs/2007.08024},
  year       = {2020},
  url        = {https://arxiv.org/abs/2007.08024},
  eprinttype = {arXiv},
  eprint     = {2007.08024},
  timestamp  = {Tue, 21 Jul 2020 12:53:33 +0200},
  biburl     = {https://dblp.org/rec/journals/corr/abs-2007-08024.bib},
  bibsource  = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{yoosuf2019fine,
  title     = {Fine-Grained Propaganda Detection with Fine-Tuned {BERT}},
  author    = {Yoosuf, Shehel  and
               Yang, Yin},
  editor    = {Feldman, Anna  and
               Da San Martino, Giovanni  and
               Barr{\'o}n-Cede{\~n}o, Alberto  and
               Brew, Chris  and
               Leberknight, Chris  and
               Nakov, Preslav},
  booktitle = {Proceedings of the Second Workshop on Natural Language Processing for Internet Freedom: Censorship, Disinformation, and Propaganda},
  month     = nov,
  year      = {2019},
  address   = {Hong Kong, China},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/D19-5011},
  doi       = {10.18653/v1/D19-5011},
  pages     = {87--91},
  abstract  = {This paper presents the winning solution of the Fragment Level Classification (FLC) task in the Fine Grained Propaganda Detection competition at the NLP4IF{'}19 workshop. The goal of the FLC task is to detect and classify textual segments that correspond to one of the 18 given propaganda techniques in a news articles dataset. The main idea of our solution is to perform word-level classification using fine-tuned BERT, a popular pre-trained language model. Besides presenting the model and its evaluation results, we also investigate the attention heads in the model, which provide insights into what the model learns, as well as aspects for potential improvements.}
}
